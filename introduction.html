
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction: Advanced Explainable AI for computer vision &#8212; Advanced AI explainability with pytorch-gradcam</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tutorial: Class Activation Maps for Semantic Segmentation" href="Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/dff1.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Advanced AI explainability with pytorch-gradcam</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Introduction: Advanced Explainable AI for computer vision
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html">
   Tutorial: Class Activation Maps for Semantic Segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Class%20Activation%20Maps%20for%20Object%20Detection%20With%20Faster%20RCNN.html">
   Tutorial: Class Activation Maps for Object Detection with Faster RCNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="EigenCAM%20for%20YOLO5.html">
   EigenCAM for YOLO5
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Pixel%20Attribution%20for%20embeddings.html">
   Tutorial: Concept Activation Maps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CAM%20Metrics%20And%20Tuning%20Tutorial.html">
   A tutorial on benchmarking and tuning model explanations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="vision_transformers.html">
   How does it work with Vision Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Deep%20Feature%20Factorizations.html">
   Deep Feature Factorizations for better model explainability
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/introduction.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction: Advanced Explainable AI for computer vision
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visual-examples">
     Visual Examples
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection-and-semantic-segmentation">
     Object Detection and Semantic Segmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explaining-similarity-to-other-images-embeddings">
     Explaining similarity to other images / embeddings
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-feature-factorization">
     Deep Feature Factorization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resnet50">
       Resnet50:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vision-transfomer-deit-tiny">
       Vision Transfomer (Deit Tiny):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#swin-transfomer-tiny-window-7-patch-4-input-size-224">
       Swin Transfomer (Tiny window:7 patch:4 input-size:224):
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics-and-evaluation-for-xai">
   Metrics and Evaluation for XAI
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chosing-the-target-layer">
   Chosing the Target Layer
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-from-code-as-a-library">
   Using from code as a library
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics-and-evaluating-the-explanations">
   Metrics and evaluating the explanations
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#smoothing-to-get-nice-looking-cams">
   Smoothing to get nice looking CAMs
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-the-example-script">
   Running the example script:
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#citation">
     Citation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction: Advanced Explainable AI for computer vision</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Introduction: Advanced Explainable AI for computer vision
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#visual-examples">
     Visual Examples
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#object-detection-and-semantic-segmentation">
     Object Detection and Semantic Segmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explaining-similarity-to-other-images-embeddings">
     Explaining similarity to other images / embeddings
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-feature-factorization">
     Deep Feature Factorization
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classification">
     Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#resnet50">
       Resnet50:
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#vision-transfomer-deit-tiny">
       Vision Transfomer (Deit Tiny):
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#swin-transfomer-tiny-window-7-patch-4-input-size-224">
       Swin Transfomer (Tiny window:7 patch:4 input-size:224):
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics-and-evaluation-for-xai">
   Metrics and Evaluation for XAI
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chosing-the-target-layer">
   Chosing the Target Layer
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-from-code-as-a-library">
   Using from code as a library
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#metrics-and-evaluating-the-explanations">
   Metrics and evaluating the explanations
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#smoothing-to-get-nice-looking-cams">
   Smoothing to get nice looking CAMs
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#running-the-example-script">
   Running the example script:
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#citation">
     Citation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="introduction-advanced-explainable-ai-for-computer-vision">
<h1>Introduction: Advanced Explainable AI for computer vision<a class="headerlink" href="#introduction-advanced-explainable-ai-for-computer-vision" title="Permalink to this headline">#</a></h1>
<p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">grad-cam</span></code></p>
<p><a class="reference external" href="https://github.com/jacobgil/pytorch-grad-cam">https://github.com/jacobgil/pytorch-grad-cam</a></p>
<p>This is a package with state of the art methods for Explainable AI for computer vision.
This can be used for diagnosing model predictions, either in production or while
developing models.
The aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.</p>
<p>⭐ Comprehensive collection of Pixel Attribution methods for Computer Vision.</p>
<p>⭐ Tested on many Common CNN Networks and Vision Transformers.</p>
<p>⭐ Advanced use cases: Works with Classification, Object Detection, Semantic Segmentation, Embedding-similarity and more.</p>
<p>⭐ Includes smoothing methods to make the CAMs look nice.</p>
<p>⭐ High performance: full support for batches of images in all methods.</p>
<p>⭐ Includes metrics for checking if you can trust the explanations, and tuning them for best performance.</p>
<p><img alt="visualization" src="https://github.com/jacobgil/jacobgil.github.io/blob/master/assets/cam_dog.gif?raw=true" /></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>What it does</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GradCAM</p></td>
<td><p>Weight the 2D activations by the average gradient</p></td>
</tr>
<tr class="row-odd"><td><p>HiResCAM</p></td>
<td><p>Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models</p></td>
</tr>
<tr class="row-even"><td><p>GradCAMElementWise</p></td>
<td><p>Like GradCAM but element-wise multiply the activations with the gradients then apply a ReLU operation before summing</p></td>
</tr>
<tr class="row-odd"><td><p>GradCAM++</p></td>
<td><p>Like GradCAM but uses second order gradients</p></td>
</tr>
<tr class="row-even"><td><p>XGradCAM</p></td>
<td><p>Like GradCAM but scale the gradients by the normalized activations</p></td>
</tr>
<tr class="row-odd"><td><p>AblationCAM</p></td>
<td><p>Zero out activations and measure how the output drops (this repository includes a fast batched implementation)</p></td>
</tr>
<tr class="row-even"><td><p>ScoreCAM</p></td>
<td><p>Perbutate the image by the scaled activations and measure how the output drops</p></td>
</tr>
<tr class="row-odd"><td><p>EigenCAM</p></td>
<td><p>Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)</p></td>
</tr>
<tr class="row-even"><td><p>EigenGradCAM</p></td>
<td><p>Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner</p></td>
</tr>
<tr class="row-odd"><td><p>LayerCAM</p></td>
<td><p>Spatially weight the activations by positive gradients. Works better especially in lower layers</p></td>
</tr>
<tr class="row-even"><td><p>FullGrad</p></td>
<td><p>Computes the gradients of the biases from all over the network, and then sums them</p></td>
</tr>
<tr class="row-odd"><td><p>Deep Feature Factorizations</p></td>
<td><p>Non Negative Matrix Factorization on the 2D activations</p></td>
</tr>
</tbody>
</table>
<div class="section" id="visual-examples">
<h2>Visual Examples<a class="headerlink" href="#visual-examples" title="Permalink to this headline">#</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>What makes the network think the image label is ‘pug, pug-dog’</p></th>
<th class="head"><p>What makes the network think the image label is ‘tabby, tabby cat’</p></th>
<th class="head"><p>Combining Grad-CAM with Guided Backpropagation for the ‘pug, pug-dog’ class</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog.jpg?raw=true" width="256" height="256"></p></td>
<td><p><img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cat.jpg?raw=true" width="256" height="256"></p></td>
<td><p><img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cam_gb_dog.jpg?raw=true" width="256" height="256"></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="object-detection-and-semantic-segmentation">
<h2>Object Detection and Semantic Segmentation<a class="headerlink" href="#object-detection-and-semantic-segmentation" title="Permalink to this headline">#</a></h2>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Object Detection</p></th>
<th class="head"><p>Semantic Segmentation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/both_detection.png?raw=true" width="256" height="256"></p></td>
<td><p><img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/cars_segmentation.png?raw=true" width="256" height="200"></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="explaining-similarity-to-other-images-embeddings">
<h2>Explaining similarity to other images / embeddings<a class="headerlink" href="#explaining-similarity-to-other-images-embeddings" title="Permalink to this headline">#</a></h2>
<img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/embeddings.png?raw=true">
</div>
<div class="section" id="deep-feature-factorization">
<h2>Deep Feature Factorization<a class="headerlink" href="#deep-feature-factorization" title="Permalink to this headline">#</a></h2>
<img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dff1.png?raw=true">
<img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dff2.png?raw=true">
</div>
<div class="section" id="classification">
<h2>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">#</a></h2>
<div class="section" id="resnet50">
<h3>Resnet50:<a class="headerlink" href="#resnet50" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>Image</p></th>
<th class="head"><p>GradCAM</p></th>
<th class="head"><p>AblationCAM</p></th>
<th class="head"><p>ScoreCAM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dog</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_dog_gradcam_cam.jpg" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_dog_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_dog_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
<tr class="row-odd"><td><p>Cat</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_cat_gradcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_cat_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/resnet50_cat_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="vision-transfomer-deit-tiny">
<h3>Vision Transfomer (Deit Tiny):<a class="headerlink" href="#vision-transfomer-deit-tiny" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>Image</p></th>
<th class="head"><p>GradCAM</p></th>
<th class="head"><p>AblationCAM</p></th>
<th class="head"><p>ScoreCAM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dog</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_dog_gradcam_cam.jpg" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_dog_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_dog_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
<tr class="row-odd"><td><p>Cat</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_cat_gradcam_cam.jpg" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_cat_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/vit_cat_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="swin-transfomer-tiny-window-7-patch-4-input-size-224">
<h3>Swin Transfomer (Tiny window:7 patch:4 input-size:224):<a class="headerlink" href="#swin-transfomer-tiny-window-7-patch-4-input-size-224" title="Permalink to this headline">#</a></h3>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>Image</p></th>
<th class="head"><p>GradCAM</p></th>
<th class="head"><p>AblationCAM</p></th>
<th class="head"><p>ScoreCAM</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Dog</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/swinT_dog_gradcam_cam.jpg" /></p></td>
<td><p><img alt="" src="examples/swinT_dog_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="examples/swinT_dog_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
<tr class="row-odd"><td><p>Cat</p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/dog_cat.jfif?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/swinT_cat_gradcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="examples/swinT_cat_ablationcam_cam.jpg?raw=true" /></p></td>
<td><p><img alt="" src="examples/swinT_cat_scorecam_cam.jpg?raw=true" /></p></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="metrics-and-evaluation-for-xai">
<h1>Metrics and Evaluation for XAI<a class="headerlink" href="#metrics-and-evaluation-for-xai" title="Permalink to this headline">#</a></h1>
<img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/metrics.png?raw=true">
<img src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/road.png?raw=true">
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="chosing-the-target-layer">
<h1>Chosing the Target Layer<a class="headerlink" href="#chosing-the-target-layer" title="Permalink to this headline">#</a></h1>
<p>You need to choose the target layer to compute CAM for.
Some common choices are:</p>
<ul class="simple">
<li><p>FasterRCNN: model.backbone</p></li>
<li><p>Resnet18 and 50: model.layer4[-1]</p></li>
<li><p>VGG and densenet161: model.features[-1]</p></li>
<li><p>mnasnet1_0: model.layers[-1]</p></li>
<li><p>ViT: model.blocks[-1].norm1</p></li>
<li><p>SwinT: model.layers[-1].blocks[-1].norm1</p></li>
</ul>
<p>If you pass a list with several layers, the CAM will be averaged accross them.
This can be useful if you’re not sure what layer will perform best.</p>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="using-from-code-as-a-library">
<h1>Using from code as a library<a class="headerlink" href="#using-from-code-as-a-library" title="Permalink to this headline">#</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_grad_cam</span> <span class="kn">import</span> <span class="n">GradCAM</span><span class="p">,</span> <span class="n">HiResCAM</span><span class="p">,</span> <span class="n">ScoreCAM</span><span class="p">,</span> <span class="n">GradCAMPlusPlus</span><span class="p">,</span> <span class="n">AblationCAM</span><span class="p">,</span> <span class="n">XGradCAM</span><span class="p">,</span> <span class="n">EigenCAM</span><span class="p">,</span> <span class="n">FullGrad</span>
<span class="kn">from</span> <span class="nn">pytorch_grad_cam.utils.model_targets</span> <span class="kn">import</span> <span class="n">ClassifierOutputTarget</span>
<span class="kn">from</span> <span class="nn">pytorch_grad_cam.utils.image</span> <span class="kn">import</span> <span class="n">show_cam_on_image</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">target_layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="o">.</span><span class="n">layer4</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="c1"># Create an input tensor image for your model..</span>
<span class="c1"># Note: input_tensor can be a batch tensor with several images!</span>

<span class="c1"># Construct the CAM object once, and then re-use it on many images:</span>
<span class="n">cam</span> <span class="o">=</span> <span class="n">GradCAM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">target_layers</span><span class="o">=</span><span class="n">target_layers</span><span class="p">,</span> <span class="n">use_cuda</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">use_cuda</span><span class="p">)</span>

<span class="c1"># You can also use it within a with statement, to make sure it is freed,</span>
<span class="c1"># In case you need to re-create it inside an outer loop:</span>
<span class="c1"># with GradCAM(model=model, target_layers=target_layers, use_cuda=args.use_cuda) as cam:</span>
<span class="c1">#   ...</span>

<span class="c1"># We have to specify the target we want to generate</span>
<span class="c1"># the Class Activation Maps for.</span>
<span class="c1"># If targets is None, the highest scoring category</span>
<span class="c1"># will be used for every image in the batch.</span>
<span class="c1"># Here we use ClassifierOutputTarget, but you can define your own custom targets</span>
<span class="c1"># That are, for example, combinations of categories, or specific outputs in a non standard model.</span>

<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="n">ClassifierOutputTarget</span><span class="p">(</span><span class="mi">281</span><span class="p">)]</span>

<span class="c1"># You can also pass aug_smooth=True and eigen_smooth=True, to apply smoothing.</span>
<span class="n">grayscale_cam</span> <span class="o">=</span> <span class="n">cam</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">=</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">targets</span><span class="p">)</span>

<span class="c1"># In this example grayscale_cam has only one image in the batch:</span>
<span class="n">grayscale_cam</span> <span class="o">=</span> <span class="n">grayscale_cam</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">visualization</span> <span class="o">=</span> <span class="n">show_cam_on_image</span><span class="p">(</span><span class="n">rgb_img</span><span class="p">,</span> <span class="n">grayscale_cam</span><span class="p">,</span> <span class="n">use_rgb</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="metrics-and-evaluating-the-explanations">
<h1>Metrics and evaluating the explanations<a class="headerlink" href="#metrics-and-evaluating-the-explanations" title="Permalink to this headline">#</a></h1>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pytorch_grad_cam.utils.model_targets</span> <span class="kn">import</span> <span class="n">ClassifierOutputSoftmaxTarget</span>
<span class="kn">from</span> <span class="nn">pytorch_grad_cam.metrics.cam_mult_image</span> <span class="kn">import</span> <span class="n">CamMultImageConfidenceChange</span>
<span class="c1"># Create the metric target, often the confidence drop in a score of some category</span>
<span class="n">metric_target</span> <span class="o">=</span> <span class="n">ClassifierOutputSoftmaxTarget</span><span class="p">(</span><span class="mi">281</span><span class="p">)</span>
<span class="n">scores</span><span class="p">,</span> <span class="n">batch_visualizations</span> <span class="o">=</span> <span class="n">CamMultImageConfidenceChange</span><span class="p">()(</span><span class="n">input_tensor</span><span class="p">,</span> 
  <span class="n">inverse_cams</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_visualization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">visualization</span> <span class="o">=</span> <span class="n">deprocess_image</span><span class="p">(</span><span class="n">batch_visualizations</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>

<span class="c1"># State of the art metric: Remove and Debias</span>
<span class="kn">from</span> <span class="nn">pytorch_grad_cam.metrics.road</span> <span class="kn">import</span> <span class="n">ROADMostRelevantFirst</span><span class="p">,</span> <span class="n">ROADLeastRelevantFirst</span>
<span class="n">cam_metric</span> <span class="o">=</span> <span class="n">ROADMostRelevantFirst</span><span class="p">(</span><span class="n">percentile</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>
<span class="n">scores</span><span class="p">,</span> <span class="n">perturbation_visualizations</span> <span class="o">=</span> <span class="n">cam_metric</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> 
  <span class="n">grayscale_cams</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_visualization</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># You can also average accross different percentiles, and combine</span>
<span class="c1"># (LeastRelevantFirst - MostRelevantFirst) / 2</span>
<span class="kn">from</span> <span class="nn">pytorch_grad_cam.metrics.road</span> <span class="kn">import</span> <span class="n">ROADMostRelevantFirstAverage</span><span class="p">,</span>
                                          <span class="n">ROADLeastRelevantFirstAverage</span><span class="p">,</span>
                                          <span class="n">ROADCombined</span>
<span class="n">cam_metric</span> <span class="o">=</span> <span class="n">ROADCombined</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">40</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">80</span><span class="p">])</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cam_metric</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">grayscale_cams</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="smoothing-to-get-nice-looking-cams">
<h1>Smoothing to get nice looking CAMs<a class="headerlink" href="#smoothing-to-get-nice-looking-cams" title="Permalink to this headline">#</a></h1>
<p>To reduce noise in the CAMs, and make it fit better on the objects,
two smoothing methods are supported:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">aug_smooth=True</span></code></p>
<p>Test time augmentation: increases the run time by x6.</p>
<p>Applies a combination of horizontal flips, and mutiplying the image
by [1.0, 1.1, 0.9].</p>
<p>This has the effect of better centering the CAM around the objects.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">eigen_smooth=True</span></code></p>
<p>First principle component of <code class="docutils literal notranslate"><span class="pre">activations*weights</span></code></p>
<p>This has the effect of removing a lot of noise.</p>
</li>
</ul>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>AblationCAM</p></th>
<th class="head"><p>aug smooth</p></th>
<th class="head"><p>eigen smooth</p></th>
<th class="head"><p>aug+eigen smooth</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/nosmooth.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/augsmooth.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/eigensmooth.jpg?raw=true" /></p></td>
<td><p><img alt="" src="https://github.com/jacobgil/pytorch-grad-cam/blob/master/examples/eigenaug.jpg?raw=true" /></p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="running-the-example-script">
<h1>Running the example script:<a class="headerlink" href="#running-the-example-script" title="Permalink to this headline">#</a></h1>
<p>Usage: <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">cam.py</span> <span class="pre">--image-path</span> <span class="pre">&lt;path_to_image&gt;</span> <span class="pre">--method</span> <span class="pre">&lt;method&gt;</span></code></p>
<p>To use with CUDA:
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">cam.py</span> <span class="pre">--image-path</span> <span class="pre">&lt;path_to_image&gt;</span> <span class="pre">--use-cuda</span></code></p>
<hr class="docutils" />
<p>You can choose between:</p>
<p><code class="docutils literal notranslate"><span class="pre">GradCAM</span></code> , <code class="docutils literal notranslate"><span class="pre">HiResCAM</span></code>, <code class="docutils literal notranslate"><span class="pre">ScoreCAM</span></code>, <code class="docutils literal notranslate"><span class="pre">GradCAMPlusPlus</span></code>, <code class="docutils literal notranslate"><span class="pre">AblationCAM</span></code>, <code class="docutils literal notranslate"><span class="pre">XGradCAM</span></code> , <code class="docutils literal notranslate"><span class="pre">LayerCAM</span></code>, <code class="docutils literal notranslate"><span class="pre">FullGrad</span></code> and <code class="docutils literal notranslate"><span class="pre">EigenCAM</span></code>.</p>
<p>Some methods like ScoreCAM and AblationCAM require a large number of forward passes,
and have a batched implementation.</p>
<p>You can control the batch size with
<code class="docutils literal notranslate"><span class="pre">cam.batch_size</span> <span class="pre">=</span> </code></p>
<hr class="docutils" />
<div class="section" id="citation">
<h2>Citation<a class="headerlink" href="#citation" title="Permalink to this headline">#</a></h2>
<p>If you use this for research, please cite. Here is an example BibTeX entry:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@misc</span><span class="p">{</span><span class="n">jacobgilpytorchcam</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">PyTorch</span> <span class="n">library</span> <span class="k">for</span> <span class="n">CAM</span> <span class="n">methods</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Jacob</span> <span class="n">Gildenblat</span> <span class="ow">and</span> <span class="n">contributors</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2021</span><span class="p">},</span>
  <span class="n">publisher</span><span class="o">=</span><span class="p">{</span><span class="n">GitHub</span><span class="p">},</span>
  <span class="n">howpublished</span><span class="o">=</span><span class="p">{</span>\<span class="n">url</span><span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">jacobgil</span><span class="o">/</span><span class="n">pytorch</span><span class="o">-</span><span class="n">grad</span><span class="o">-</span><span class="n">cam</span><span class="p">}},</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="tex2jax_ignore mathjax_ignore section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/1610.02391">https://arxiv.org/abs/1610.02391</a> <br>
<code class="docutils literal notranslate"><span class="pre">Grad-CAM:</span> <span class="pre">Visual</span> <span class="pre">Explanations</span> <span class="pre">from</span> <span class="pre">Deep</span> <span class="pre">Networks</span> <span class="pre">via</span> <span class="pre">Gradient-based</span> <span class="pre">Localization</span> <span class="pre">Ramprasaath</span> <span class="pre">R.</span> <span class="pre">Selvaraju,</span> <span class="pre">Michael</span> <span class="pre">Cogswell,</span> <span class="pre">Abhishek</span> <span class="pre">Das,</span> <span class="pre">Ramakrishna</span> <span class="pre">Vedantam,</span> <span class="pre">Devi</span> <span class="pre">Parikh,</span> <span class="pre">Dhruv</span> <span class="pre">Batra</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/2011.08891">https://arxiv.org/abs/2011.08891</a> <br>
<code class="docutils literal notranslate"><span class="pre">Use</span> <span class="pre">HiResCAM</span> <span class="pre">instead</span> <span class="pre">of</span> <span class="pre">Grad-CAM</span> <span class="pre">for</span> <span class="pre">faithful</span> <span class="pre">explanations</span> <span class="pre">of</span> <span class="pre">convolutional</span> <span class="pre">neural</span> <span class="pre">networks</span> <span class="pre">Rachel</span> <span class="pre">L.</span> <span class="pre">Draelos,</span> <span class="pre">Lawrence</span> <span class="pre">Carin</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/1710.11063">https://arxiv.org/abs/1710.11063</a> <br>
<code class="docutils literal notranslate"><span class="pre">Grad-CAM++:</span> <span class="pre">Improved</span> <span class="pre">Visual</span> <span class="pre">Explanations</span> <span class="pre">for</span> <span class="pre">Deep</span> <span class="pre">Convolutional</span> <span class="pre">Networks</span> <span class="pre">Aditya</span> <span class="pre">Chattopadhyay,</span> <span class="pre">Anirban</span> <span class="pre">Sarkar,</span> <span class="pre">Prantik</span> <span class="pre">Howlader,</span> <span class="pre">Vineeth</span> <span class="pre">N</span> <span class="pre">Balasubramanian</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/1910.01279">https://arxiv.org/abs/1910.01279</a> <br>
<code class="docutils literal notranslate"><span class="pre">Score-CAM:</span> <span class="pre">Score-Weighted</span> <span class="pre">Visual</span> <span class="pre">Explanations</span> <span class="pre">for</span> <span class="pre">Convolutional</span> <span class="pre">Neural</span> <span class="pre">Networks</span> <span class="pre">Haofan</span> <span class="pre">Wang,</span> <span class="pre">Zifan</span> <span class="pre">Wang,</span> <span class="pre">Mengnan</span> <span class="pre">Du,</span> <span class="pre">Fan</span> <span class="pre">Yang,</span> <span class="pre">Zijian</span> <span class="pre">Zhang,</span> <span class="pre">Sirui</span> <span class="pre">Ding,</span> <span class="pre">Piotr</span> <span class="pre">Mardziel,</span> <span class="pre">Xia</span> <span class="pre">Hu</span></code></p>
<p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/9093360/">https://ieeexplore.ieee.org/abstract/document/9093360/</a> <br>
<code class="docutils literal notranslate"><span class="pre">Ablation-cam:</span> <span class="pre">Visual</span> <span class="pre">explanations</span> <span class="pre">for</span> <span class="pre">deep</span> <span class="pre">convolutional</span> <span class="pre">network</span> <span class="pre">via</span> <span class="pre">gradient-free</span> <span class="pre">localization.</span> <span class="pre">Saurabh</span> <span class="pre">Desai</span> <span class="pre">and</span> <span class="pre">Harish</span> <span class="pre">G</span> <span class="pre">Ramaswamy.</span> <span class="pre">In</span> <span class="pre">WACV,</span> <span class="pre">pages</span> <span class="pre">972–980,</span> <span class="pre">2020</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/2008.02312">https://arxiv.org/abs/2008.02312</a> <br>
<code class="docutils literal notranslate"><span class="pre">Axiom-based</span> <span class="pre">Grad-CAM:</span> <span class="pre">Towards</span> <span class="pre">Accurate</span> <span class="pre">Visualization</span> <span class="pre">and</span> <span class="pre">Explanation</span> <span class="pre">of</span> <span class="pre">CNNs</span> <span class="pre">Ruigang</span> <span class="pre">Fu,</span> <span class="pre">Qingyong</span> <span class="pre">Hu,</span> <span class="pre">Xiaohu</span> <span class="pre">Dong,</span> <span class="pre">Yulan</span> <span class="pre">Guo,</span> <span class="pre">Yinghui</span> <span class="pre">Gao,</span> <span class="pre">Biao</span> <span class="pre">Li</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/2008.00299">https://arxiv.org/abs/2008.00299</a> <br>
<code class="docutils literal notranslate"><span class="pre">Eigen-CAM:</span> <span class="pre">Class</span> <span class="pre">Activation</span> <span class="pre">Map</span> <span class="pre">using</span> <span class="pre">Principal</span> <span class="pre">Components</span> <span class="pre">Mohammed</span> <span class="pre">Bany</span> <span class="pre">Muhammad,</span> <span class="pre">Mohammed</span> <span class="pre">Yeasin</span></code></p>
<p><a class="reference external" href="http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf">http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf</a> <br>
<code class="docutils literal notranslate"><span class="pre">LayerCAM:</span> <span class="pre">Exploring</span> <span class="pre">Hierarchical</span> <span class="pre">Class</span> <span class="pre">Activation</span> <span class="pre">Maps</span> <span class="pre">for</span> <span class="pre">Localization</span> <span class="pre">Peng-Tao</span> <span class="pre">Jiang;</span> <span class="pre">Chang-Bin</span> <span class="pre">Zhang;</span> <span class="pre">Qibin</span> <span class="pre">Hou;</span> <span class="pre">Ming-Ming</span> <span class="pre">Cheng;</span> <span class="pre">Yunchao</span> <span class="pre">Wei</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/1905.00780">https://arxiv.org/abs/1905.00780</a> <br>
<code class="docutils literal notranslate"><span class="pre">Full-Gradient</span> <span class="pre">Representation</span> <span class="pre">for</span> <span class="pre">Neural</span> <span class="pre">Network</span> <span class="pre">Visualization</span> <span class="pre">Suraj</span> <span class="pre">Srinivas,</span> <span class="pre">Francois</span> <span class="pre">Fleuret</span></code></p>
<p><a class="reference external" href="https://arxiv.org/abs/1806.10206">https://arxiv.org/abs/1806.10206</a> <br>
<code class="docutils literal notranslate"><span class="pre">Deep</span> <span class="pre">Feature</span> <span class="pre">Factorization</span> <span class="pre">For</span> <span class="pre">Concept</span> <span class="pre">Discovery</span> <span class="pre">Edo</span> <span class="pre">Collins,</span> <span class="pre">Radhakrishna</span> <span class="pre">Achanta,</span> <span class="pre">Sabine</span> <span class="pre">Süsstrunk</span></code></p>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='right-next' id="next-link" href="Class%20Activation%20Maps%20for%20Semantic%20Segmentation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tutorial: Class Activation Maps for Semantic Segmentation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Jacob Gildenblat<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>